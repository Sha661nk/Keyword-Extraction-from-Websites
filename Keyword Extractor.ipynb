{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50ab83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shashank\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shashank\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import yake \n",
    "import time\n",
    "import pandas as pd\n",
    "from summa import keywords as sk\n",
    "from keybert import KeyBERT\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "stop_word = ['parser', 'nav', 'sdk', 'input', 'font', 'output', \n",
    "             'div', 'cookie', 'policy', 'banner', 'onetrust',\n",
    "             'navigation', 'margin', 'height', 'left', 'top']\n",
    "\n",
    "stop_words = set(stopwords.words('english') + stop_word)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "tag_re = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8e33b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper():\n",
    "    document = []\n",
    "    query = \"Data Mining\" \n",
    "    for search_results in search(query, tld=\"com\", num=10, stop=10, pause=2):\n",
    "        try:\n",
    "            html = urlopen(search_results).read()\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            document.append(soup)\n",
    "        except:\n",
    "            pass\n",
    "    return (document)\n",
    "\n",
    "def cleaner(x): \n",
    "    brackets_removed = tag_re.sub('', str(x))\n",
    "    new_line_removed = str(brackets_removed).replace(r'\\n', ' ')\n",
    "    email_removed = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', ' ', new_line_removed)\n",
    "    symbols_removed = re.sub('[^A-Za-z0-9]+', ' ', email_removed)\n",
    "    clean_data = re.sub(r\"(^|\\W)\\d+\", ' ', symbols_removed)  \n",
    "    clean_data = lemmatizer.lemmatize(clean_data)\n",
    "    clean_data = ps.stem(clean_data)\n",
    "    clean_data = clean_data.replace('\\n','')\n",
    "    return clean_data\n",
    "\n",
    "def keyword_extractor(method, corpus, top_n, max_ngram_size):\n",
    "\n",
    "    try:\n",
    "        if method == 'keybert':\n",
    "            keybert_keywords = kw_model.extract_keywords(corpus, \n",
    "                                         keyphrase_ngram_range=(1, max_ngram_size), \n",
    "                                         stop_words='english', \n",
    "                                         highlight=False,\n",
    "                                         top_n=10)\n",
    "            return list(dict(keybert_keywords).keys())\n",
    "\n",
    "        elif method == 'text_rank':\n",
    "            summa_keywords = list(sk.keywords(corpus, scores=True))\n",
    "            summa_keys = [word[0] for word in summa_keywords[0:top_n]]\n",
    "            return summa_keys\n",
    "\n",
    "        elif method == 'yake':\n",
    "            kw_yake = yake.KeywordExtractor(top=top_n, stopwords=stop_words, n=max_ngram_size)\n",
    "            yake_keywords = kw_yake.extract_keywords(corpus)\n",
    "            yake_keys = [word[0] for word in yake_keywords]\n",
    "            return yake_keys\n",
    "\n",
    "    except:\n",
    "        print('Error: Key word extraction method not found!')\n",
    "\n",
    "def driver(top_n_words, grams, method):\n",
    "    document = keyword_extraction.scrapper()\n",
    "    ngrams  = []\n",
    "    for doc in range(len(document)):\n",
    "        ngrams.append(keyword_extractor(method, cleaner(document[doc]), top_n_words, grams))\n",
    "\n",
    "    n_keywords = [ngram for ngram_list in ngrams for ngram in ngram_list]\n",
    "    n_keywords = list(filter(lambda x: set(x.split(' ')).isdisjoint(stop_word), n_keywords))\n",
    "\n",
    "    keys = [[common_keywords, n_keywords.count(common_keywords)] for common_keywords in set(n_keywords)]  \n",
    "    keyword_freq = pd.DataFrame(keys, columns=['Keyword', 'Freq']).sort_values(by='Freq', ascending=False).reset_index(drop=True)\n",
    "    keyword_freq = keyword_freq[keyword_freq[\"Freq\"] > 3]\n",
    "    print(query)\n",
    "\n",
    "    return keyword_freq.to_json('keyword.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9c26622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'a', 't', 'a', ' ', 'M', 'i', 'n', 'i', 'n', 'g']\n",
      "--- 34.67284679412842 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "driver(top_n_words = 100, grams = 3, method = 'yake')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_extractor('keybert', clean[0], 10)\n",
    "#keyword_extractor('text_rank', clean[doc], 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
