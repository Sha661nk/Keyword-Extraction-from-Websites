{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50ab83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shashank\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shashank\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import yake \n",
    "import time\n",
    "import pandas as pd\n",
    "from summa import keywords as sk\n",
    "from keybert import KeyBERT\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "stop_words = set(stopwords.words('english') + stop_word)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "tag_re = re.compile(r'<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8e33b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper(query_string, num_of_pages, stop_at):\n",
    "    document = []\n",
    "    for search_results in search(query_string, tld=\"com\", num=num_of_pages, stop=stop_at, pause=2):\n",
    "        try:\n",
    "            html = urlopen(search_results).read()\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            document.append(soup.body)\n",
    "        except:\n",
    "            pass\n",
    "    return (document)\n",
    "\n",
    "def cleaner(x): \n",
    "    brackets_removed = tag_re.sub('', str(x))\n",
    "    new_line_removed = str(brackets_removed).replace(r'\\n', ' ')\n",
    "    email_removed = re.sub(r'[A-Za-z0-9]*@[A-Za-z]*\\.?[A-Za-z0-9]*', ' ', new_line_removed)\n",
    "    symbols_removed = re.sub('[^A-Za-z0-9]+', ' ', email_removed)\n",
    "    clean_data = re.sub(r\"(^|\\W)\\d+\", ' ', symbols_removed)  \n",
    "    clean_data = lemmatizer.lemmatize(clean_data)\n",
    "    clean_data = ps.stem(clean_data)\n",
    "    clean_data = clean_data.replace('\\n','')\n",
    "    return clean_data\n",
    "\n",
    "def keyword_extractor(method, corpus, top_n, max_ngram_size):\n",
    "\n",
    "    try:\n",
    "        if method == 'keybert':\n",
    "            keybert_keywords = kw_model.extract_keywords(corpus, \n",
    "                                         keyphrase_ngram_range=(1, max_ngram_size), \n",
    "                                         stop_words='english', \n",
    "                                         highlight=False,\n",
    "                                         top_n=10)\n",
    "            return list(dict(keybert_keywords).keys())\n",
    "\n",
    "        elif method == 'text_rank':\n",
    "            summa_keywords = list(sk.keywords(corpus, scores=True))\n",
    "            summa_keys = [word[0] for word in summa_keywords[0:top_n]]\n",
    "            return summa_keys\n",
    "\n",
    "        elif method == 'yake':\n",
    "            kw_yake = yake.KeywordExtractor(top=top_n, stopwords=stop_words, n=max_ngram_size)\n",
    "            yake_keywords = kw_yake.extract_keywords(corpus)\n",
    "            yake_keys = [word[0] for word in yake_keywords]\n",
    "            return yake_keys\n",
    "\n",
    "    except:\n",
    "        print('Error: Key word extraction method not found!')\n",
    "\n",
    "def driver(query_string, num_of_pages, stop_at, top_n_words, grams, method):\n",
    "    data = []\n",
    "    for query in list(query_string):\n",
    "        document = scrapper(query, num_of_pages, stop_at)\n",
    "        ngrams  = []\n",
    "        for doc in range(len(document)):\n",
    "            ngrams.append(keyword_extractor(method, cleaner(document[doc]), top_n_words, grams))\n",
    "\n",
    "        n_keywords = [ngram for ngram_list in ngrams for ngram in ngram_list]\n",
    "        n_keywords = list(filter(lambda x: set(x.split(' ')).isdisjoint(stop_words), n_keywords))\n",
    "\n",
    "        keys = [[common_keywords, n_keywords.count(common_keywords)] for common_keywords in set(n_keywords)]  \n",
    "        keyword_freq = pd.DataFrame(keys, columns=['Keyword', 'Freq']).sort_values(by='Freq', ascending=False).reset_index(drop=True)\n",
    "        keyword_freq = keyword_freq[keyword_freq[\"Freq\"] > 3]\n",
    "        keyword_freq['Google Search Query'] = query\n",
    "        data.append(keyword_freq)\n",
    "\n",
    "    final_keys = pd.concat(data)\n",
    "    return final_keys.to_json('keyword.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9c26622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 55.909438610076904 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "driver(query_string = ['Data Mining', 'Machine Learning'], num_of_pages = 10, stop_at = 10, top_n_words = 100, grams = 3, method = 'yake')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d2f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_extractor('keybert', clean[0], 10)\n",
    "#keyword_extractor('text_rank', clean[doc], 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
